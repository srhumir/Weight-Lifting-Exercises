---
title: "Weight lifting Analysis"
author: "Reza"
date: "29 April 2016"
output: html_document
---

#Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in five different ways. One totally right and the four others with a common mistake.
We will predict how well a given person done the job using these accelerometers.

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

#Synopsis
The provided dataset contains the the name of voluonteer, time stamp, `r 160-7-1-96` measurment by accelometers, 96 statistica calculated by moving window on them and the class of actvity, "A" for totally right and "B-E" for four commom mistakes. As we are supposed to predict the class of activity based on sigle measurments by acceleometers, we can not use statistics. So we are limited to `r 160-7-1-96` measurments. These are still too many variables and make the trainig too slow. To reduce the variables. We first ommit the highly correlated variables. Then train a classification tree on the remaining data. Finally choose the most imortant variables based on this model. Finally train a random forest model on the remaining variables. The final out of sample accuracy based on 50% of data for trainig and 50% for testing is 97%. We did not use cross validation as there are enough data (aroung 20 thousand) in the trainig set.

#Details of doing the job
First of all the data are downloaded and loaded into R
```{r message=FALSE, cache=TRUE}
library(caret)
library(reshape)
#downloading and reading the data
if (!file.exists("pml-trainig.csv")){
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                      destfile = "pml-trainig.csv")
}
if (!file.exists("pml-testing.csv")){
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                      destfile = "pml-testing.csv")
}
roughtrainig <- read.csv("pml-trainig.csv")
roughtesting <- read.csv("pml-testing.csv")
```
```{r echo=FALSE}
library(caret)
```
Then I took a look at variables names.
```{r }
names(roughtrainig)[1:10]
```

The names of variables contiaing statisctics started with one of the wrods, "max", "min", "stddev", "avg", "amplitude", "kurtosis","skewness" or "var". So I check which columns contain these data and removed them.
```{r}
#Getting the field with statistics
indlist <- sapply(list("max", "min", "stddev", "avg", "amplitude", "kurtosis","skewness", "^var"),
              function(s) grep(s, names(roughtrainig)))
ind <-vector() 
for (i in 1:length(indlist)){
        ind <- c(ind, indlist[[i]])
}
data <- roughtrainig[,-ind]
```
Then I removed the forst seven columns which do not actually contain measurments. 
```{r}
#removing first 7 columns
data <- data[, -c(1:7)]
```
The variables need to be converted to numeric. As they might be saved as character.
```{r}
#converting all data to numeric
for (i in 1:(dim(data)[2]-1)){
        data[,i] <- as.numeric(data[,i])
}
```
From this step I start summerizing the data. First I remove variables which are highly correlated to others. "caret" package has tow very usefull functions for this, "cor" which produce a correlation matrix. and "findCorrelation" which cut too correlated variables.
```{r}
#removing features which are highly correlated
correlationMatrix <- cor(data[,-dim(data)[2]])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=.8)
data <- data[,-highlyCorrelated]
```
`r length(highlyCorrelated)` variables are removed leaving `r dim(data)[2]-1` variables. But it is still too much for runnig a random forest. First divide the dataset to trainig and testing subsets half of data to each subset.
```{r}
#divide to trian and test
set.seed(1234)
indtrain <- createDataPartition(data$classe, p = .5, list = F)
training <- data[indtrain,]
testing <- data[-indtrain,]
```
To reduce the number of variales, one idea would be using PCA. I tried that too but did not get better result. So I decided to use another method. 

As the other approach I used the variable importance function in "caret". I firstly trained a decision tree model on all the data which by the way did not show a good out of sample accuracy.
```{r}
modelTree <- train(classe~., data=training, method="rpart")
```
Then ran the varImp to get the first 20 important variables.
```{r}
varImp(modelTree)
```
I choosed the variables which have 25% or more importance.
```{r}
Impvars2 <- row.names(varImp(modelTree)$importance)[varImp(modelTree)$importance >25]
```
Leave us by `r length(Impvar2)` variables. 

Now I seperate just these `r length(Impvar2)` variables and train a random forest model on them.
```{R cache=TRUE}
trainingRF <- training[,Impvars2]
modelRF <- train(trainingRF, training$classe, method = "rf")
```
